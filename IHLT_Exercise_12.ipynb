{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghufranullah1997/IHLT_notebooks/blob/main/IHLT_Exercise_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 12: analogy evaluation\n",
        "\n",
        "In the lecture, we touched upon *Mikolov's analogy dataset* which was one of the first analogy evaluation datasets for word embeddings. It consists of 9+5=14 sets of word analogies. You can find it for example here: https://github.com/nicholas-leonard/word2vec/blob/master/questions-words.txt\n",
        "\n",
        "It might be interesting to know how well our embeddings fare on each of these 14 tasks. And that will be our exercise. The steps are as follows:\n",
        "\n",
        "1. Read in the analogy tuples from the file above, for each task separately (the format of the file is kinda self-explanatory once you open it)\n",
        "2. Write a function `eval_analogy(tuples,embeddings,K)` which will return the top-K accuracy of the `embeddings` (Gensim's KeyedVectors) on `tuples`, which are the analogy 4-tuples. For instance for the tuple (\"Athens\",\"Greece\",\"Havana\",\"Cuba\") will be counted as correct if the analogy on the first three words results in K nearest neighbors that contain also \"Cuba\". Hope this is clear. :)\n",
        "3. Run this function on the 14 tasks you read in step (1) and see if you see any interesting differences\n",
        "\n",
        "Below is the relevant embedding-loading and analogy example code from the lecture that you can reuse.\n",
        "\n",
        "**Tip:** these do take a while to compute, so you might want to debug your code on a small sample and when happy, run the whole thing only once. I also like to use `tqdm` to get a progress bar, so I see how long I need to wait to see some output."
      ],
      "metadata": {
        "id": "AeYKb5I5bAeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_KYfCs1DsM1",
        "outputId": "f7230557-e2aa-4eb0-ee74-89919d0653f7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix binary incompatibility issues in Colab\n",
        "!pip install --upgrade --force-reinstall numpy scipy pandas gensim\n",
        "\n",
        "# Restart the runtime to reload libraries properly\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vTTfM3iAD_qa",
        "outputId": "c3b7aaf7-568f-4ae4-bf18-d253ac82f836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting scipy\n",
            "  Using cached scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Collecting gensim\n",
            "  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas)\n",
            "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting numpy\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting scipy\n",
            "  Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting smart-open>=1.8.1 (from gensim)\n",
            "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
            "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
            "  Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "Installing collected packages: pytz, wrapt, tzdata, six, numpy, smart-open, scipy, python-dateutil, pandas, gensim\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.3\n",
            "    Uninstalling pandas-2.2.3:\n",
            "      Successfully uninstalled pandas-2.2.3\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.3\n",
            "    Uninstalling gensim-4.3.3:\n",
            "      Successfully uninstalled gensim-4.3.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 pandas-2.2.3 python-dateutil-2.9.0.post0 pytz-2025.2 scipy-1.13.1 six-1.17.0 smart-open-7.1.0 tzdata-2025.2 wrapt-1.17.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "six"
                ]
              },
              "id": "02f55e71d55c4ba488cf09f3467dcb8c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c3-FPp5MZjAS"
      },
      "outputs": [],
      "source": [
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I found this link in the NLPL repository\n",
        "# It refers to English model trained on the Gigaword corpus of news\n",
        "##!wget http://vectors.nlpl.eu/repository/20/12.zip\n",
        "\n",
        "## Try these if the download above is too slow, I mirrored these:\n",
        "!wget http://dl.turkunlp.org/TKO_7095_2023/12.zip\n",
        "#!wget http://dl.turkunlp.org/TKO_7095_2023/42.zip\n"
      ],
      "metadata": {
        "id": "QRbTPYZGZuiJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "024b0ab0-43c6-4fec-8425-c9fdb7125705"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-06 19:47:01--  http://dl.turkunlp.org/TKO_7095_2023/12.zip\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 613577258 (585M) [application/zip]\n",
            "Saving to: ‘12.zip.1’\n",
            "\n",
            "12.zip.1            100%[===================>] 585.15M  11.6MB/s    in 36s     \n",
            "\n",
            "2025-05-06 19:47:37 (16.4 MB/s) - ‘12.zip.1’ saved [613577258/613577258]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Somewhat awkwardly, these are numbered files and both\n",
        "# .zip files contain \"model.bin\"\n",
        "# Let's unzip and rename\n",
        "# -o means \"do not ask, overwrite by default\"\n",
        "!unzip -o 12.zip\n",
        "!mv model.bin en.bin\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiV_3crkzBtI",
        "outputId": "6c670143-8ebc-4a1e-fb03-58a0262e7300"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  12.zip\n",
            "  inflating: meta.json               \n",
            "  inflating: model.bin               \n",
            "  inflating: model.txt               \n",
            "  inflating: README                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Now we can load the embeddings\n",
        "*   These are huge, but they are sorted by frequency, so we can easily limit ourselves to the top 100,000 words, which will be plenty enough for us\n",
        "*   This is maybe good to note, now we enter the territory of NLP models which count in the gigabytes in size\n",
        "\n"
      ],
      "metadata": {
        "id": "lL1rXODZOCYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is how you load the trained embeddings\n",
        "# check the documentation\n",
        "# w2v embeddings are traditionlly distributed in one of two formats: a text form, and a binary form\n",
        "# The embeddings we downloaded above are in the binary form, so we need to indicate that when loading\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "wv_emb_en=KeyedVectors.load_word2vec_format(\"en.bin\", limit=100000, binary=True)\n"
      ],
      "metadata": {
        "id": "4DbFt1VOaDCu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`KeyedVectors` documentation is here: https://radimrehurek.com/gensim/models/keyedvectors.html"
      ],
      "metadata": {
        "id": "mEsD37OxeP2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic operations with the embeddings\n",
        "\n",
        "* The KeyedVectors object allows for all the basic operations with embeddings which we saw in the lecture\n"
      ],
      "metadata": {
        "id": "FT9CdAlCOW3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word analogy\n",
        "\n",
        "* \"A is to B as C is to D\"\n",
        "* Can be implemented as D=B-A+C, where (A,B,C) are word embeddings\n",
        "* Then list words nearest to the computed embedding D\n",
        "* In the library, the implementation lets us list words with \"+\" sign, and words with \"-\" sign\n"
      ],
      "metadata": {
        "id": "XtyrScWuOokr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# B     A      C\n",
        "# Paris-France+Sweden= ___?\n",
        "#\n",
        "# i.e. France is to Paris as Sweden is to X\n",
        "wv_emb_en.most_similar(positive=[\"Paris\",\"Sweden\"],negative=[\"France\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PaVLPxsOujq",
        "outputId": "7cb4f6a1-2cad-4898-d153-3837f5390449"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Stockholm', 0.7338932752609253),\n",
              " ('Malmo', 0.5458161234855652),\n",
              " ('Helsinki', 0.5444940328598022),\n",
              " ('Goteborg', 0.5421050190925598),\n",
              " ('Swedish', 0.5309098362922668),\n",
              " ('Malmoe', 0.5198634266853333),\n",
              " ('Oslo', 0.5004472732543945),\n",
              " ('Gothenburg', 0.4957912266254425),\n",
              " ('STOCKHOLM', 0.48791587352752686),\n",
              " ('Copenhagen', 0.47769418358802795)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "triples=[(\"cow\",\"milk\",\"hen\"),\n",
        "         (\"Paris\",\"France\",\"Helsinki\"),\n",
        "         (\"car\",\"wheel\",\"airplane\"),\n",
        "         (\"airplane\",\"propeller\",\"ship\"),\n",
        "         (\"king\",\"queen\",\"man\"),\n",
        "         (\"man\",\"doctor\",\"woman\"),\n",
        "         (\"man\",\"boss\",\"woman\")\n",
        "         ]\n",
        "for what,is_to_what,as_this_is in triples:\n",
        "    # is_to_what-what+as_this_is\n",
        "    to_what=wv_emb_en.most_similar(positive=[is_to_what,as_this_is],negative=[what])[0][0]\n",
        "    print(f\"{what} is to {is_to_what} as {as_this_is} is to: {to_what}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-85ZWiCmPVv9",
        "outputId": "ddffbc6e-5314-4bc3-fc4a-f060c2e21eaf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cow is to milk as hen is to: sauce\n",
            "Paris is to France as Helsinki is to: Finland\n",
            "car is to wheel as airplane is to: rudder\n",
            "airplane is to propeller as ship is to: vessel\n",
            "king is to queen as man is to: woman\n",
            "man is to doctor as woman is to: physician\n",
            "man is to boss as woman is to: bosses\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The exercise code starts below\n",
        "\n",
        "* I will donate you a function for reading Mikolov's data, but I recommend you delete it and write your own as a further exercise\n",
        "* Reading annoying file formats is an integral part of NLP"
      ],
      "metadata": {
        "id": "BuMn_21DlLTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remember you always need to download the \"raw\" link from GitHub, or else you get an HTML with the pretty-printed data, not the data itself\n",
        "!wget https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt"
      ],
      "metadata": {
        "id": "CBx6TuWtliB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tasks={} #A dictionary with taskname as key, and value will then be a list of 4-tuples with the analogy data\n",
        "\n",
        "with open(\"questions-words.txt\") as f:\n",
        "    for line in f:\n",
        "        line=line.rstrip(\"\\n\")\n",
        "        if not line:\n",
        "            continue #skip possible empty lines\n",
        "        if line.startswith(\": \"): #All tasks seem to start with a line like \": task-name\"\n",
        "            taskname=line[2:] #get rid of \": \"\n",
        "            tuple_list=[] #let's make a new list for the tuples and store it into the tasks dictionary\n",
        "            #then we keep filling it, until a new task starts, when a new list is created, the previous\n",
        "            #of course remains in the `tasks` dictionary\n",
        "            tasks[taskname]=tuple_list\n",
        "        else: #not a task line, so this should be a 4-word line, with words space-separated it seems\n",
        "            w1,w2,w3,w4=line.split()\n",
        "            tuple_list.append((w1,w2,w3,w4)) #let's append it and we're done\n",
        "\n",
        "print(f\"We have {len(tasks)} tasks.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iT4EFRMUlwoS",
        "outputId": "f62f9fb2-adf1-4d5a-9de4-c4d9a52a8a1b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 14 tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "def eval_analogy(tuples,embeddings,K):\n",
        "    #### YOUR CODE GOES HERE ########\n",
        "\n",
        "### MY results are\n",
        "# Task *gram9-plural-verbs* has top-3 accuracy of 83.45%\n",
        "# Task *capital-common-countries* has top-3 accuracy of 93.68%\n",
        "# Task *capital-world* has top-3 accuracy of 95.97%\n",
        "# Task *currency* has top-3 accuracy of 40.54%\n",
        "# Task *city-in-state* has top-3 accuracy of 63.76%\n",
        "# Task *family* has top-3 accuracy of 93.16%\n",
        "# Task *gram1-adjective-to-adverb* has top-3 accuracy of 49.25%\n",
        "# Task *gram2-opposite* has top-3 accuracy of 49.62%\n",
        "# Task *gram3-comparative* has top-3 accuracy of 95.50%\n",
        "# Task *gram4-superlative* has top-3 accuracy of 86.32%\n",
        "# Task *gram5-present-participle* has top-3 accuracy of 83.97%\n",
        "# Task *gram6-nationality-adjective* has top-3 accuracy of 95.45%\n",
        "# Task *gram7-past-tense* has top-3 accuracy of 90.45%\n",
        "# Task *gram8-plural* has top-3 accuracy of 89.04%\n",
        "# Task *gram9-plural-verbs* has top-3 accuracy of 83.45%"
      ],
      "metadata": {
        "id": "ZkD_Ky24nvqn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "afb56d6e-ad8d-4961-f1b8-74c0eee2dbe3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-8-1043452d90a2>, line 21)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-1043452d90a2>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    # Task *gram9-plural-verbs* has top-3 accuracy of 83.45%\u001b[0m\n\u001b[0m                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "def evaluate_analogies(pairs_list, model, top_k):\n",
        "    correct_matches = 0   # For counting of correct analogies\n",
        "    total_checked = 0     # Total evaluated (excluding skipped ones)\n",
        "\n",
        "    # Go through each 4-word analogy tuple\n",
        "    for word_a, word_b, word_c, expected_word in tqdm.tqdm(pairs_list):\n",
        "        try:\n",
        "            # Predict: word_b is to word_a as ? is to word_c\n",
        "            predictions = model.most_similar(positive=[word_b, word_c], negative=[word_a], topn=top_k)\n",
        "\n",
        "            # Extract only the predicted words from (word, similarity) tuples\n",
        "            predicted_words = set(w for w, _ in predictions)\n",
        "\n",
        "            # Check if expected answer is among the top-k predictions\n",
        "            if expected_word in predicted_words:\n",
        "                correct_matches += 1\n",
        "\n",
        "            total_checked += 1\n",
        "\n",
        "        except KeyError:\n",
        "            # Skipping the tuple if any word is missing from the vocabulary\n",
        "            continue\n",
        "\n",
        "    # Return accuracy as a percentage\n",
        "    return correct_matches / total_checked * 100 if total_checked > 0 else 0\n",
        "\n",
        "\n",
        "# Store (task name, accuracy) results\n",
        "evaluation_results = []\n",
        "\n",
        "# Number of top candidates to consider in each analogy prediction\n",
        "top_k_value = 3\n",
        "\n",
        "# Evaluate each task in the analogy set\n",
        "for name, analogy_tuples in tasks.items():\n",
        "    task_accuracy = evaluate_analogies(analogy_tuples, wv_emb_en, top_k_value)\n",
        "    evaluation_results.append((name, task_accuracy))\n",
        "    print(f\"Task *{name}* has top-{top_k_value} accuracy of {task_accuracy:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXcD919-Eq0Q",
        "outputId": "0ebc637c-4b7a-4d9a-97bc-5edf0b1ea2ff"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 506/506 [00:10<00:00, 47.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task *capital-common-countries* has top-3 accuracy of 93.68%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4524/4524 [01:17<00:00, 58.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task *capital-world* has top-3 accuracy of 95.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 866/866 [00:11<00:00, 74.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task *currency* has top-3 accuracy of 40.54%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2467/2467 [00:50<00:00, 49.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task *city-in-state* has top-3 accuracy of 63.76%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 506/506 [00:07<00:00, 67.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task *family* has top-3 accuracy of 93.16%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 992/992 [00:17<00:00, 58.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task *gram1-adjective-to-adverb* has top-3 accuracy of 49.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 812/812 [00:15<00:00, 52.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task *gram2-opposite* has top-3 accuracy of 49.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1332/1332 [00:27<00:00, 48.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task *gram3-comparative* has top-3 accuracy of 95.50%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1122/1122 [00:21<00:00, 53.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task *gram4-superlative* has top-3 accuracy of 86.32%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1056/1056 [00:19<00:00, 54.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task *gram5-present-participle* has top-3 accuracy of 83.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1599/1599 [00:31<00:00, 51.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task *gram6-nationality-adjective* has top-3 accuracy of 95.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1560/1560 [00:35<00:00, 43.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task *gram7-past-tense* has top-3 accuracy of 90.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1332/1332 [00:29<00:00, 45.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task *gram8-plural* has top-3 accuracy of 89.04%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 870/870 [00:19<00:00, 45.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task *gram9-plural-verbs* has top-3 accuracy of 83.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0xtacZHSErhw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}